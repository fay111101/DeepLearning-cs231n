{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C7: Develop Your First Neural Network With Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Overview\n",
    "\n",
    "Six steps:\n",
    "\n",
    "1. Load Data.\n",
    "2. Define Model.\n",
    "3. Compile Model.\n",
    "4. Fit Model.\n",
    "5. Evaluate Model.\n",
    "6. Tie It All Together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Get Data Set\n",
    "\n",
    "In this tutorial we are going to use the Pima Indians onset of diabetes dataset.\n",
    "\n",
    "The introduction of this data set: [pima-indians-diabetes.names](http://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.names)\n",
    "\n",
    "The data set file: [pima-indians-diabetes.data](http://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data)\n",
    "\n",
    "You can get the data by the fellowing step:\n",
    "\n",
    "```\n",
    "$ cd ./data_set/\n",
    "$ ./get_pima_indians_diabetes_data.sh\n",
    "```\n",
    "\n",
    "pima-indians-diabetes.data will be downloaded at data_set/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Load Data\n",
    "\n",
    "### Initialize random seed\n",
    "\n",
    "It is a good idea to initialize the random number generator with a fixed seed value. This is so that you can run the same code again and again and get the same result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data\n",
    "\n",
    "Load data by numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (768, 8) float64\n",
      "Y (768,) float64\n"
     ]
    }
   ],
   "source": [
    "dataset = np.loadtxt(\"./data_set/pima-indians-diabetes.data\", delimiter=',')\n",
    "X = dataset[:, 0:8]\n",
    "Y = dataset[:, 8]\n",
    "print 'X', X.shape, X.dtype\n",
    "print 'Y', Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Define Model\n",
    "\n",
    "Create a model by keras.models.Sequential and add the layers we designed:\n",
    "\n",
    "    The first hidden layer has 12 neurons and expects 8 input variables, and a relu activation.\n",
    "    The second hidden layer has 8 neurons, and a relu activation.\n",
    "    Finally the output layer has 1 neuron to predict the class, and a sigmoid activation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, init='uniform', activation='relu'))\n",
    "model.add(Dense(8, init='uniform', activation='relu'))\n",
    "model.add(Dense(1, init='uniform', activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Compile Model\n",
    "\n",
    "Compiling the model uses the efficient numercial libraries (Tensorflow or Theano). In this step, we shoule specify some hyperperemeters for training process:\n",
    "\n",
    "    loss function: binary_crossentropy\n",
    "    optimizer: adam\n",
    "    metrics\" accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 Fit Model\n",
    "\n",
    "Fit moel means training model, the peremeters in this step are:\n",
    "\n",
    "    nb_epoch: 150\n",
    "    batch_size: 10\n",
    "\n",
    "nb_epoch means the number of epoch, which fix the number of iterations. batch_size means the batch size in the method \"mini-batch gradient descent\"\n",
    "\n",
    "The training process is runing on your CPU or GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "768/768 [==============================] - 0s - loss: 0.4553 - acc: 0.7812     \n",
      "Epoch 2/150\n",
      "768/768 [==============================] - 0s - loss: 0.4560 - acc: 0.7878     \n",
      "Epoch 3/150\n",
      "768/768 [==============================] - 0s - loss: 0.4608 - acc: 0.7930     \n",
      "Epoch 4/150\n",
      "768/768 [==============================] - 0s - loss: 0.4546 - acc: 0.7852     \n",
      "Epoch 5/150\n",
      "768/768 [==============================] - 0s - loss: 0.4567 - acc: 0.7839     \n",
      "Epoch 6/150\n",
      "768/768 [==============================] - 0s - loss: 0.4527 - acc: 0.7891     \n",
      "Epoch 7/150\n",
      "768/768 [==============================] - 0s - loss: 0.4586 - acc: 0.7813     \n",
      "Epoch 8/150\n",
      "768/768 [==============================] - 0s - loss: 0.4521 - acc: 0.7839     \n",
      "Epoch 9/150\n",
      "768/768 [==============================] - 0s - loss: 0.4544 - acc: 0.7812     \n",
      "Epoch 10/150\n",
      "768/768 [==============================] - 0s - loss: 0.4508 - acc: 0.7917     \n",
      "Epoch 11/150\n",
      "768/768 [==============================] - 0s - loss: 0.4477 - acc: 0.7812     \n",
      "Epoch 12/150\n",
      "768/768 [==============================] - 0s - loss: 0.4566 - acc: 0.7917     \n",
      "Epoch 13/150\n",
      "768/768 [==============================] - 0s - loss: 0.4492 - acc: 0.7839     \n",
      "Epoch 14/150\n",
      "768/768 [==============================] - 0s - loss: 0.4510 - acc: 0.7943     \n",
      "Epoch 15/150\n",
      "768/768 [==============================] - 0s - loss: 0.4531 - acc: 0.7891     \n",
      "Epoch 16/150\n",
      "768/768 [==============================] - 0s - loss: 0.4574 - acc: 0.7865     \n",
      "Epoch 17/150\n",
      "768/768 [==============================] - 0s - loss: 0.4587 - acc: 0.7917     \n",
      "Epoch 18/150\n",
      "768/768 [==============================] - 0s - loss: 0.4609 - acc: 0.7734     \n",
      "Epoch 19/150\n",
      "768/768 [==============================] - 0s - loss: 0.4642 - acc: 0.7760     \n",
      "Epoch 20/150\n",
      "768/768 [==============================] - 0s - loss: 0.4490 - acc: 0.7878     \n",
      "Epoch 21/150\n",
      "768/768 [==============================] - 0s - loss: 0.4572 - acc: 0.7799     \n",
      "Epoch 22/150\n",
      "768/768 [==============================] - 0s - loss: 0.4469 - acc: 0.7956     \n",
      "Epoch 23/150\n",
      "768/768 [==============================] - 0s - loss: 0.4505 - acc: 0.7812     \n",
      "Epoch 24/150\n",
      "768/768 [==============================] - 0s - loss: 0.4461 - acc: 0.7917     \n",
      "Epoch 25/150\n",
      "768/768 [==============================] - 0s - loss: 0.4466 - acc: 0.8047     \n",
      "Epoch 26/150\n",
      "768/768 [==============================] - 0s - loss: 0.4498 - acc: 0.7930     \n",
      "Epoch 27/150\n",
      "768/768 [==============================] - 0s - loss: 0.4536 - acc: 0.7852     \n",
      "Epoch 28/150\n",
      "768/768 [==============================] - 0s - loss: 0.4466 - acc: 0.7917     \n",
      "Epoch 29/150\n",
      "768/768 [==============================] - 0s - loss: 0.4528 - acc: 0.7917     \n",
      "Epoch 30/150\n",
      "768/768 [==============================] - 0s - loss: 0.4441 - acc: 0.8008     \n",
      "Epoch 31/150\n",
      "768/768 [==============================] - 0s - loss: 0.4421 - acc: 0.7969     \n",
      "Epoch 32/150\n",
      "768/768 [==============================] - 0s - loss: 0.4615 - acc: 0.7695     \n",
      "Epoch 33/150\n",
      "768/768 [==============================] - 0s - loss: 0.4452 - acc: 0.8034     \n",
      "Epoch 34/150\n",
      "768/768 [==============================] - 0s - loss: 0.4496 - acc: 0.7904     \n",
      "Epoch 35/150\n",
      "768/768 [==============================] - 0s - loss: 0.4490 - acc: 0.8073     \n",
      "Epoch 36/150\n",
      "768/768 [==============================] - 0s - loss: 0.4497 - acc: 0.7982     \n",
      "Epoch 37/150\n",
      "768/768 [==============================] - 0s - loss: 0.4438 - acc: 0.8073     \n",
      "Epoch 38/150\n",
      "768/768 [==============================] - 0s - loss: 0.4421 - acc: 0.8021     \n",
      "Epoch 39/150\n",
      "768/768 [==============================] - 0s - loss: 0.4455 - acc: 0.7904     \n",
      "Epoch 40/150\n",
      "768/768 [==============================] - 0s - loss: 0.4469 - acc: 0.7930     \n",
      "Epoch 41/150\n",
      "768/768 [==============================] - 0s - loss: 0.4437 - acc: 0.7865     \n",
      "Epoch 42/150\n",
      "768/768 [==============================] - 0s - loss: 0.4496 - acc: 0.7826     \n",
      "Epoch 43/150\n",
      "768/768 [==============================] - 0s - loss: 0.4406 - acc: 0.7878     \n",
      "Epoch 44/150\n",
      "768/768 [==============================] - 0s - loss: 0.4507 - acc: 0.7812     \n",
      "Epoch 45/150\n",
      "768/768 [==============================] - 0s - loss: 0.4404 - acc: 0.7930     \n",
      "Epoch 46/150\n",
      "768/768 [==============================] - 0s - loss: 0.4495 - acc: 0.7839     \n",
      "Epoch 47/150\n",
      "768/768 [==============================] - 0s - loss: 0.4478 - acc: 0.7734     \n",
      "Epoch 48/150\n",
      "768/768 [==============================] - 0s - loss: 0.4477 - acc: 0.7995     \n",
      "Epoch 49/150\n",
      "768/768 [==============================] - 0s - loss: 0.4417 - acc: 0.7995     \n",
      "Epoch 50/150\n",
      "768/768 [==============================] - 0s - loss: 0.4496 - acc: 0.7865     \n",
      "Epoch 51/150\n",
      "768/768 [==============================] - 0s - loss: 0.4429 - acc: 0.7839     \n",
      "Epoch 52/150\n",
      "768/768 [==============================] - 0s - loss: 0.4481 - acc: 0.7865     \n",
      "Epoch 53/150\n",
      "768/768 [==============================] - 0s - loss: 0.4506 - acc: 0.7995     \n",
      "Epoch 54/150\n",
      "768/768 [==============================] - 0s - loss: 0.4489 - acc: 0.7878     \n",
      "Epoch 55/150\n",
      "768/768 [==============================] - 0s - loss: 0.4442 - acc: 0.7956     \n",
      "Epoch 56/150\n",
      "768/768 [==============================] - 0s - loss: 0.4482 - acc: 0.7930     \n",
      "Epoch 57/150\n",
      "768/768 [==============================] - 0s - loss: 0.4447 - acc: 0.7930     \n",
      "Epoch 58/150\n",
      "768/768 [==============================] - 0s - loss: 0.4447 - acc: 0.7943     \n",
      "Epoch 59/150\n",
      "768/768 [==============================] - 0s - loss: 0.4434 - acc: 0.8099     \n",
      "Epoch 60/150\n",
      "768/768 [==============================] - 0s - loss: 0.4458 - acc: 0.7969     \n",
      "Epoch 61/150\n",
      "768/768 [==============================] - 0s - loss: 0.4473 - acc: 0.7904     \n",
      "Epoch 62/150\n",
      "768/768 [==============================] - 0s - loss: 0.4417 - acc: 0.7930     \n",
      "Epoch 63/150\n",
      "768/768 [==============================] - 0s - loss: 0.4388 - acc: 0.7995     \n",
      "Epoch 64/150\n",
      "768/768 [==============================] - 0s - loss: 0.4511 - acc: 0.7943     \n",
      "Epoch 65/150\n",
      "768/768 [==============================] - 0s - loss: 0.4481 - acc: 0.7930     \n",
      "Epoch 66/150\n",
      "768/768 [==============================] - 0s - loss: 0.4656 - acc: 0.7917     \n",
      "Epoch 67/150\n",
      "768/768 [==============================] - 0s - loss: 0.4365 - acc: 0.7982     \n",
      "Epoch 68/150\n",
      "768/768 [==============================] - 0s - loss: 0.4453 - acc: 0.7917     \n",
      "Epoch 69/150\n",
      "768/768 [==============================] - 0s - loss: 0.4373 - acc: 0.7865     \n",
      "Epoch 70/150\n",
      "768/768 [==============================] - 0s - loss: 0.4339 - acc: 0.8034     \n",
      "Epoch 71/150\n",
      "768/768 [==============================] - 0s - loss: 0.4348 - acc: 0.8034     \n",
      "Epoch 72/150\n",
      "768/768 [==============================] - 0s - loss: 0.4375 - acc: 0.7956     \n",
      "Epoch 73/150\n",
      "768/768 [==============================] - 0s - loss: 0.4390 - acc: 0.7982     \n",
      "Epoch 74/150\n",
      "768/768 [==============================] - 0s - loss: 0.4375 - acc: 0.7969     \n",
      "Epoch 75/150\n",
      "768/768 [==============================] - 0s - loss: 0.4558 - acc: 0.7943     \n",
      "Epoch 76/150\n",
      "768/768 [==============================] - 0s - loss: 0.4335 - acc: 0.8047     \n",
      "Epoch 77/150\n",
      "768/768 [==============================] - 0s - loss: 0.4414 - acc: 0.7930     \n",
      "Epoch 78/150\n",
      "768/768 [==============================] - 0s - loss: 0.4378 - acc: 0.8021     \n",
      "Epoch 79/150\n",
      "768/768 [==============================] - 0s - loss: 0.4501 - acc: 0.7930     \n",
      "Epoch 80/150\n",
      "768/768 [==============================] - 0s - loss: 0.4387 - acc: 0.8008     \n",
      "Epoch 81/150\n",
      "768/768 [==============================] - 0s - loss: 0.4349 - acc: 0.7943     \n",
      "Epoch 82/150\n",
      "768/768 [==============================] - 0s - loss: 0.4445 - acc: 0.7878     \n",
      "Epoch 83/150\n",
      "768/768 [==============================] - 0s - loss: 0.4428 - acc: 0.7891     \n",
      "Epoch 84/150\n",
      "768/768 [==============================] - 0s - loss: 0.4366 - acc: 0.7865     \n",
      "Epoch 85/150\n",
      "768/768 [==============================] - 0s - loss: 0.4400 - acc: 0.7852     \n",
      "Epoch 86/150\n",
      "768/768 [==============================] - 0s - loss: 0.4396 - acc: 0.8008     \n",
      "Epoch 87/150\n",
      "768/768 [==============================] - 0s - loss: 0.4463 - acc: 0.7969     \n",
      "Epoch 88/150\n",
      "768/768 [==============================] - 0s - loss: 0.4357 - acc: 0.7878     \n",
      "Epoch 89/150\n",
      "768/768 [==============================] - 0s - loss: 0.4387 - acc: 0.7969     \n",
      "Epoch 90/150\n",
      "768/768 [==============================] - 0s - loss: 0.4480 - acc: 0.7956     \n",
      "Epoch 91/150\n",
      "768/768 [==============================] - 0s - loss: 0.4336 - acc: 0.7943     \n",
      "Epoch 92/150\n",
      "768/768 [==============================] - 0s - loss: 0.4402 - acc: 0.7930     \n",
      "Epoch 93/150\n",
      "768/768 [==============================] - 0s - loss: 0.4485 - acc: 0.7891     \n",
      "Epoch 94/150\n",
      "768/768 [==============================] - 0s - loss: 0.4379 - acc: 0.7904     \n",
      "Epoch 95/150\n",
      "768/768 [==============================] - 0s - loss: 0.4360 - acc: 0.7982     \n",
      "Epoch 96/150\n",
      "768/768 [==============================] - 0s - loss: 0.4373 - acc: 0.7995     \n",
      "Epoch 97/150\n",
      "768/768 [==============================] - 0s - loss: 0.4356 - acc: 0.7878     \n",
      "Epoch 98/150\n",
      "768/768 [==============================] - 0s - loss: 0.4343 - acc: 0.7943     \n",
      "Epoch 99/150\n",
      "768/768 [==============================] - 0s - loss: 0.4373 - acc: 0.7995     \n",
      "Epoch 100/150\n",
      "768/768 [==============================] - 0s - loss: 0.4329 - acc: 0.8060     \n",
      "Epoch 101/150\n",
      "768/768 [==============================] - 0s - loss: 0.4345 - acc: 0.7878     \n",
      "Epoch 102/150\n",
      "768/768 [==============================] - 0s - loss: 0.4368 - acc: 0.7982     \n",
      "Epoch 103/150\n",
      "768/768 [==============================] - 0s - loss: 0.4676 - acc: 0.7721     \n",
      "Epoch 104/150\n",
      "768/768 [==============================] - 0s - loss: 0.4439 - acc: 0.7995     \n",
      "Epoch 105/150\n",
      "768/768 [==============================] - 0s - loss: 0.4433 - acc: 0.7982     \n",
      "Epoch 106/150\n",
      "768/768 [==============================] - 0s - loss: 0.4305 - acc: 0.7982     \n",
      "Epoch 107/150\n",
      "768/768 [==============================] - 0s - loss: 0.4295 - acc: 0.7969     \n",
      "Epoch 108/150\n",
      "768/768 [==============================] - 0s - loss: 0.4332 - acc: 0.7930     \n",
      "Epoch 109/150\n",
      "768/768 [==============================] - 0s - loss: 0.4302 - acc: 0.8008     \n",
      "Epoch 110/150\n",
      "768/768 [==============================] - 0s - loss: 0.4403 - acc: 0.7956     \n",
      "Epoch 111/150\n",
      "768/768 [==============================] - 0s - loss: 0.4430 - acc: 0.7956     \n",
      "Epoch 112/150\n",
      "768/768 [==============================] - 0s - loss: 0.4408 - acc: 0.7930     \n",
      "Epoch 113/150\n",
      "768/768 [==============================] - 0s - loss: 0.4254 - acc: 0.7969     \n",
      "Epoch 114/150\n",
      "768/768 [==============================] - 0s - loss: 0.4372 - acc: 0.7878     \n",
      "Epoch 115/150\n",
      "768/768 [==============================] - 0s - loss: 0.4341 - acc: 0.7982     \n",
      "Epoch 116/150\n",
      "768/768 [==============================] - 0s - loss: 0.4347 - acc: 0.7969     \n",
      "Epoch 117/150\n",
      "768/768 [==============================] - 0s - loss: 0.4372 - acc: 0.7891     \n",
      "Epoch 118/150\n",
      "768/768 [==============================] - 0s - loss: 0.4272 - acc: 0.8008     \n",
      "Epoch 119/150\n",
      "768/768 [==============================] - 0s - loss: 0.4368 - acc: 0.8021     \n",
      "Epoch 120/150\n",
      "768/768 [==============================] - 0s - loss: 0.4372 - acc: 0.7943     \n",
      "Epoch 121/150\n",
      "768/768 [==============================] - 0s - loss: 0.4310 - acc: 0.8086     \n",
      "Epoch 122/150\n",
      "768/768 [==============================] - 0s - loss: 0.4279 - acc: 0.8047     \n",
      "Epoch 123/150\n",
      "768/768 [==============================] - 0s - loss: 0.4308 - acc: 0.8073     \n",
      "Epoch 124/150\n",
      "768/768 [==============================] - 0s - loss: 0.4394 - acc: 0.7969     \n",
      "Epoch 125/150\n",
      "768/768 [==============================] - 0s - loss: 0.4319 - acc: 0.8021     \n",
      "Epoch 126/150\n",
      "768/768 [==============================] - 0s - loss: 0.4406 - acc: 0.7969     \n",
      "Epoch 127/150\n",
      "768/768 [==============================] - 0s - loss: 0.4337 - acc: 0.7956     \n",
      "Epoch 128/150\n",
      "768/768 [==============================] - 0s - loss: 0.4295 - acc: 0.7982     \n",
      "Epoch 129/150\n",
      "768/768 [==============================] - 0s - loss: 0.4362 - acc: 0.7917     \n",
      "Epoch 130/150\n",
      "768/768 [==============================] - 0s - loss: 0.4298 - acc: 0.7852     \n",
      "Epoch 131/150\n",
      "768/768 [==============================] - 0s - loss: 0.4326 - acc: 0.7982     \n",
      "Epoch 132/150\n",
      "768/768 [==============================] - 0s - loss: 0.4411 - acc: 0.7904     \n",
      "Epoch 133/150\n",
      "768/768 [==============================] - 0s - loss: 0.4271 - acc: 0.8073     \n",
      "Epoch 134/150\n",
      "768/768 [==============================] - 0s - loss: 0.4385 - acc: 0.7904     \n",
      "Epoch 135/150\n",
      "768/768 [==============================] - 0s - loss: 0.4314 - acc: 0.8073     \n",
      "Epoch 136/150\n",
      "768/768 [==============================] - 0s - loss: 0.4376 - acc: 0.7917     \n",
      "Epoch 137/150\n",
      "768/768 [==============================] - 0s - loss: 0.4408 - acc: 0.7865     \n",
      "Epoch 138/150\n",
      "768/768 [==============================] - 0s - loss: 0.4327 - acc: 0.7969     \n",
      "Epoch 139/150\n",
      "768/768 [==============================] - 0s - loss: 0.4313 - acc: 0.7969     \n",
      "Epoch 140/150\n",
      "768/768 [==============================] - 0s - loss: 0.4316 - acc: 0.7930     \n",
      "Epoch 141/150\n",
      "768/768 [==============================] - 0s - loss: 0.4344 - acc: 0.7982     \n",
      "Epoch 142/150\n",
      "768/768 [==============================] - 0s - loss: 0.4335 - acc: 0.7969     \n",
      "Epoch 143/150\n",
      "768/768 [==============================] - 0s - loss: 0.4335 - acc: 0.8047     \n",
      "Epoch 144/150\n",
      "768/768 [==============================] - 0s - loss: 0.4260 - acc: 0.8008     \n",
      "Epoch 145/150\n",
      "768/768 [==============================] - 0s - loss: 0.4380 - acc: 0.7930     \n",
      "Epoch 146/150\n",
      "768/768 [==============================] - 0s - loss: 0.4219 - acc: 0.7995     \n",
      "Epoch 147/150\n",
      "768/768 [==============================] - 0s - loss: 0.4289 - acc: 0.7982     \n",
      "Epoch 148/150\n",
      "768/768 [==============================] - 0s - loss: 0.4444 - acc: 0.7826     \n",
      "Epoch 149/150\n",
      "768/768 [==============================] - 0s - loss: 0.4372 - acc: 0.7969     \n",
      "Epoch 150/150\n",
      "768/768 [==============================] - 0s - loss: 0.4396 - acc: 0.7878     \n",
      "Fit time Cost 11.503775835 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "# fit model\n",
    "model.fit(X, Y, nb_epoch=150, batch_size=10)\n",
    "end_time = time.time()\n",
    "print \"Fit time Cost %s s\"%(end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 7.7 Evaluate Model\n",
    "\n",
    "In this part, we can calculate the accuracy fo this model on training dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/768 [>.............................] - ETA: 0sTraining Dataset loss: 0.80\n",
      "Training Dataset acc: 79.56%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "scores = model.evaluate(X, Y)\n",
    "print \"Training Dataset %s: %.2f\"%(model.metrics_names[0], scores[1])\n",
    "print \"Training Dataset %s: %.2f%%\"%(model.metrics_names[1], scores[1]*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 7.8 Switch to GPU model\n",
    "\n",
    "\n",
    "### 7.8.1 For MacOS with Nvidia GPU\n",
    "\n",
    "[mac osx/linux下如何将keras运行在GPU上](http://blog.csdn.net/u014205968/article/details/50166651)\n",
    "\n",
    "Note: there are some question when install CUDA if your xcode version is 8.0 above, see [here](http://blog.cycleuser.org/use-cuda-80-with-macos-sierra-1012.html)\n",
    "\n",
    "使用下面这个脚本来验证是否启动GPU:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Elemwise{exp,no_inplace}(<TensorType(float64, vector)>)]\n",
      "Looping 1000 times took 2.961800 seconds\n",
      "Result is [ 1.23178032  1.61879341  1.52278065 ...,  2.20771815  2.29967753\n",
      "  1.62323285]\n",
      "Used the cpu\n"
     ]
    }
   ],
   "source": [
    "from theano import function, config, shared, sandbox  \n",
    "import theano.tensor as T  \n",
    "import numpy  \n",
    "import time  \n",
    "  \n",
    "vlen = 10 * 30 * 768  # 10 x #cores x # threads per core  \n",
    "iters = 1000  \n",
    "  \n",
    "rng = numpy.random.RandomState(22)  \n",
    "x = shared(numpy.asarray(rng.rand(vlen), config.floatX))  \n",
    "f = function([], T.exp(x))  \n",
    "print(f.maker.fgraph.toposort())  \n",
    "t0 = time.time()  \n",
    "for i in xrange(iters):  \n",
    "    r = f()  \n",
    "t1 = time.time()  \n",
    "print(\"Looping %d times took %f seconds\" % (iters, t1 - t0))  \n",
    "print(\"Result is %s\" % (r,))  \n",
    "if numpy.any([isinstance(x.op, T.Elemwise) for x in f.maker.fgraph.toposort()]):  \n",
    "    print('Used the cpu')  \n",
    "else:  \n",
    "    print('Used the gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
